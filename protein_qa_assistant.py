from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
import streamlit as st
from langchain.chains import ConversationalRetrievalChain
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Tongyi
import re
import hashlib
import uuid
from langchain.prompts import PromptTemplate

# Set API Key
os.environ["DASHSCOPE_API_KEY"] = "your_api_key"

# Set up web page
st.title("ğŸ§¬ è›‹ç™½è´¨ç ”ç©¶é—®ç­”åŠ©æ‰‹")
st.write("ä¸Šä¼ è›‹ç™½è´¨ç ”ç©¶ç›¸å…³çš„PDFæ–‡çŒ®ï¼Œè·å–ä¸“ä¸šè§£ç­”")

# Initialize session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'documents_processed' not in st.session_state:
    st.session_state.documents_processed = False
if 'query' not in st.session_state:
    st.session_state.query = ""
if 'uploaded_files_hash' not in st.session_state:
    st.session_state.uploaded_files_hash = ""
if 'processed_documents' not in st.session_state:
    st.session_state.processed_documents = {}
if 'document_info' not in st.session_state:
    st.session_state.document_info = {}
if 'qa' not in st.session_state:
    st.session_state.qa = None
if 'docsearch' not in st.session_state:
    st.session_state.docsearch = None
if 'input_key' not in st.session_state:
    st.session_state.input_key = str(uuid.uuid4())
if 'show_question_in_input' not in st.session_state:
    st.session_state.show_question_in_input = False
if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = []
if 'new_answer' not in st.session_state: 
    st.session_state.new_answer = None
if 'new_query' not in st.session_state:  
    st.session_state.new_query = None
if 'new_sources' not in st.session_state:
    st.session_state.new_sources = None

# Clear history button
def clear_chat_history():
    st.session_state.chat_history = []
    st.session_state.query = ""
    st.session_state.input_key = str(uuid.uuid4())
    st.session_state.show_question_in_input = False
    st.session_state.new_answer = None
    st.session_state.new_query = None
    st.session_state.new_sources = None
    st.success("é—®ç­”è®°å½•å·²æ¸…é™¤")

# Sidebar function buttons
st.sidebar.title("æ“ä½œé¢æ¿")

# Clear chat history button
if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤é—®ç­”è®°å½•"):
    clear_chat_history()

# Common protein research questions
PROTEIN_QUESTIONS = [
    "è›‹ç™½è´¨çš„ç»“æ„å±‚æ¬¡æœ‰å“ªäº›ï¼Ÿ",
    "ç®€è¿°è›‹ç™½è´¨æŠ˜å çš„è¿‡ç¨‹",
    "è›‹ç™½è´¨ç»„å­¦çš„ç ”ç©¶æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ",
    "è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹çš„ä¸»è¦æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ",
    "è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨çš„ç ”ç©¶æŠ€æœ¯",
    "è›‹ç™½è´¨ç»“æ„é¢„æµ‹çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ",
    "è›‹ç™½è´¨å·¥ç¨‹çš„ä¸»è¦åº”ç”¨é¢†åŸŸ",
    "è§£é‡Šè›‹ç™½è´¨å˜æ€§å’Œå¤æ€§çš„è¿‡ç¨‹",
    "è›‹ç™½è´¨å®šé‡åˆ†æçš„å¸¸ç”¨æ–¹æ³•",
    "è›‹ç™½è´¨çº¯åŒ–æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ"
]

# Display common questions in sidebar
st.sidebar.divider()
st.sidebar.title("å¸¸è§è›‹ç™½è´¨ç ”ç©¶é—®é¢˜")
for i, question in enumerate(PROTEIN_QUESTIONS):
    if st.sidebar.button(question, key=f"q_{i}"):
        st.session_state.query = question
        st.session_state.show_question_in_input = True
        st.session_state.input_key = str(uuid.uuid4())

# Set up multiple PDF file upload
uploaded_files = st.file_uploader("ä¸Šä¼ è›‹ç™½è´¨ç ”ç©¶ç›¸å…³çš„PDFæ–‡çŒ®", 
                                 type="pdf", 
                                 accept_multiple_files=True)

# Save uploaded files to session_state
if uploaded_files:
    st.session_state.uploaded_files = uploaded_files

def clean_text(text):
    """Clean illegal characters and surrogate pairs from text"""
    text = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', text)
    try:
        return text.encode('utf-8', 'surrogatepass').decode('utf-8', 'ignore')
    except:
        return text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')

def extract_text_from_pdf(pdf_reader, file_name):
    """Extract text from PDF and add document information"""
    raw_text = ""
    for i, page in enumerate(pdf_reader.pages):
        text = page.extract_text()
        if text:
            raw_text += f"æ–‡æ¡£[{file_name}] ç¬¬{i+1}é¡µ: {clean_text(text)}\n\n"
    return raw_text

def get_file_hash(uploaded_files):
    """Generate unique hash value for uploaded files"""
    if not uploaded_files:
        return ""
    
    hasher = hashlib.sha256()
    for file in uploaded_files:
        hasher.update(file.name.encode('utf-8'))
        hasher.update(file.getvalue())
    return hasher.hexdigest()

# Check for new file uploads
current_hash = get_file_hash(st.session_state.uploaded_files)
if current_hash != st.session_state.uploaded_files_hash:
    st.session_state.documents_processed = False
    st.session_state.uploaded_files_hash = current_hash

if st.session_state.uploaded_files and not st.session_state.documents_processed:
    with st.spinner("æ­£åœ¨å¤„ç†è›‹ç™½è´¨æ–‡çŒ®..."):
        try:
            all_texts = []
            document_info = {}
            
            for file_idx, uploaded_file in enumerate(st.session_state.uploaded_files):
                if uploaded_file.name in st.session_state.processed_documents:
                    raw_text = st.session_state.processed_documents[uploaded_file.name]
                else:
                    doc_reader = PdfReader(uploaded_file)
                    raw_text = extract_text_from_pdf(doc_reader, uploaded_file.name)
                    st.session_state.processed_documents[uploaded_file.name] = raw_text
                
                text_splitter = CharacterTextSplitter(
                    separator="\n\n",
                    chunk_size=500,
                    chunk_overlap=100
                )
                texts = text_splitter.split_text(raw_text)
                all_texts.extend(texts)
                
                document_info[file_idx] = {
                    "name": uploaded_file.name,
                    "pages": len(doc_reader.pages) if 'doc_reader' in locals() else "å·²å¤„ç†"
                }
            
            EMBEDDING_MODEL = "/share/org/BGI/bgi_wangdt/liuqifan/chatglm/KQA/m3e-base"
            embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
            
            docsearch = FAISS.from_texts(all_texts, embeddings)
            st.session_state.docsearch = docsearch
            
            custom_prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è›‹ç™½è´¨ç ”ç©¶åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”ç”¨æˆ·å…³äºä¸Šä¼ æ–‡çŒ®çš„é—®é¢˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
            å¦‚æœé—®é¢˜ä¸ä¸Šä¸‹æ–‡æ— å…³ï¼Œæˆ–è€…ä¸Šä¸‹æ–‡æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·å›ç­”"è¿™ä¸ªé—®é¢˜ä¸ä¸Šä¼ çš„æ–‡çŒ®æ— å…³ï¼Œæˆ‘æ— æ³•å›ç­”"ã€‚
            å³ä½¿ä½ çŸ¥é“ç­”æ¡ˆï¼Œä½†å¦‚æœç­”æ¡ˆæ²¡æœ‰åœ¨æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®æåŠï¼Œä¹Ÿä¸è¦å›ç­”ã€‚
            è¯·ä½¿ç”¨ä¸“ä¸šã€å‡†ç¡®çš„è¯­è¨€å›ç­”è›‹ç™½è´¨ç ”ç©¶ç›¸å…³é—®é¢˜ã€‚
            
            ä¸Šä¸‹æ–‡:
            {context}
            
            é—®é¢˜: {question}
            å›ç­”:"""
            
            CUSTOM_QUESTION_PROMPT = PromptTemplate(
                input_variables=["context", "question"], 
                template=custom_prompt_template
            )
            
            st.session_state.qa = ConversationalRetrievalChain.from_llm(
                llm=Tongyi(model_name="qwen-max"),
                retriever=docsearch.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": 5}
                ),
                return_source_documents=True,
                combine_docs_chain_kwargs={"prompt": CUSTOM_QUESTION_PROMPT}
            )
            
            st.session_state.documents_processed = True
            st.session_state.document_info = document_info
            
            st.success(f"æˆåŠŸå¤„ç† {len(st.session_state.uploaded_files)} ä¸ªæ–‡çŒ®!")
            for file_idx, info in document_info.items():
                st.info(f"ğŸ“„ {info['name']} - {info['pages']}é¡µ")
            
        except Exception as e:
            st.error(f"å¤„ç†æ–‡çŒ®æ—¶å‡ºé”™: {str(e)}")

# If documents are processed, display Q&A interface
if st.session_state.get('documents_processed', False):
    # Display document information
    st.sidebar.divider()
    st.sidebar.subheader("å·²ä¸Šä¼ æ–‡çŒ®")
    for file_idx, info in st.session_state.document_info.items():
        st.sidebar.info(f"ğŸ“„ {info['name']} ({info['pages']}é¡µ)")
    
    # Display chat history
    if st.session_state.chat_history:
        st.subheader("é—®ç­”å†å²")
        for i, (query, answer, sources) in enumerate(st.session_state.chat_history):
            st.markdown(f"**é—®é¢˜ {i+1}:** {query}")
            st.markdown(f"**å›ç­” {i+1}:** {answer}")
            
            if sources:
                with st.expander(f"æŸ¥çœ‹æ¥æºä¿¡æ¯ (é—®é¢˜ {i+1})"):
                    for j, source in enumerate(sources):
                        st.text_area(
                            label=f"æ¥æº {i+1}-{j+1}",
                            value=source, 
                            height=100, 
                            key=f"source_{i}_{j}", 
                            label_visibility="collapsed"
                        )
            
            st.divider()
    
    # Display newly generated Q&A
    if st.session_state.new_query and st.session_state.new_answer:
        st.subheader(f"é—®é¢˜: {st.session_state.new_query}")
        st.subheader("ä¸“ä¸šè§£ç­”:")
        st.write(st.session_state.new_answer)
        
        if st.session_state.new_sources:
            with st.expander("æŸ¥çœ‹æ¥æºæ–‡çŒ®"):
                for j, source in enumerate(st.session_state.new_sources):
                    st.text_area(
                        label=f"å½“å‰æ¥æº {j+1}", 
                        value=source, 
                        height=100, 
                        label_visibility="collapsed"
                    )
        
        # Add to history
        st.session_state.chat_history.append(
            (st.session_state.new_query, st.session_state.new_answer, st.session_state.new_sources)
        )
        
        # Reset new Q&A state
        st.session_state.new_query = None
        st.session_state.new_answer = None
        st.session_state.new_sources = None
    
    # Get user query
    input_value = st.session_state.query if st.session_state.show_question_in_input else ""
    
    query = st.text_input(
        "è¾“å…¥æ‚¨å…³äºè›‹ç™½è´¨ç ”ç©¶çš„é—®é¢˜", 
        value=input_value,  # Display question based on state
        key=st.session_state.input_key,  # Use unique key
    )
    # Add a generate button
    generate_button = st.button("ç”Ÿæˆç­”æ¡ˆ")
    
    if (generate_button and query) or (st.session_state.show_question_in_input and query):
        with st.spinner("æ­£åœ¨æ£€ç´¢æ–‡çŒ®å¹¶ç”Ÿæˆä¸“ä¸šç­”æ¡ˆ..."):
            try:
                # Pass the question and chat history to the conversation chain to get model output
                result = st.session_state.qa({"question": query, "chat_history": st.session_state.chat_history})
                answer = result["answer"]
                source_documents = result['source_documents']
                
                # Extract source information
                source_info = []
                if source_documents:
                    for doc in source_documents[:3]:  # Display up to 3 sources
                        if "æ–‡æ¡£[" in doc.page_content and "]" in doc.page_content:
                            doc_name = doc.page_content.split("æ–‡æ¡£[", 1)[1].split("]", 1)[0]
                            page_info = doc.page_content.split("ç¬¬", 1)[1].split("é¡µ", 1)[0] if "ç¬¬" in doc.page_content else "æœªçŸ¥é¡µç "
                            content = doc.page_content.split(":", 1)[1] if ":" in doc.page_content else doc.page_content
                            
                            source_info.append(f"ğŸ“„ {doc_name} (ç¬¬{page_info}é¡µ): {clean_text(content)[:250]}...")
                        else:
                            source_info.append(clean_text(doc.page_content)[:250] + "...")
                
                # Store new Q&A results
                st.session_state.new_query = query
                st.session_state.new_answer = answer
                st.session_state.new_sources = source_info
                
                # Clear input box
                st.session_state.query = ""
                st.session_state.input_key = str(uuid.uuid4())
                st.session_state.show_question_in_input = False
                
                # Rerun to display new Q&A
                st.experimental_rerun()
                
            except Exception as e:
                st.error(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}")

# Protein research resources
st.sidebar.divider()
st.sidebar.subheader("è›‹ç™½è´¨ç ”ç©¶èµ„æº")
st.sidebar.markdown("[UniProt è›‹ç™½è´¨æ•°æ®åº“](https://www.uniprot.org/)")
st.sidebar.markdown("[PDB è›‹ç™½è´¨ç»“æ„æ•°æ®åº“](https://www.rcsb.org/)")
st.sidebar.markdown("[Protein Data Bank](https://www.wwpdb.org/)")
st.sidebar.markdown("[NCBI è›‹ç™½è´¨èµ„æº](https://www.ncbi.nlm.nih.gov/protein)")

# Introduction to protein research
st.sidebar.divider()
st.sidebar.title("è›‹ç™½è´¨ç ”ç©¶ç®€ä»‹")
st.sidebar.write("""
è›‹ç™½è´¨æ˜¯ç”Ÿå‘½æ´»åŠ¨çš„ä¸»è¦æ‰§è¡Œè€…ï¼Œå‚ä¸ç»†èƒç»“æ„æ„å»ºã€ä»£è°¢è°ƒæ§ã€ä¿¡å·ä¼ å¯¼ç­‰å…³é”®ç”Ÿç‰©è¿‡ç¨‹ã€‚
**ä¸»è¦ç ”ç©¶æ–¹å‘**:
- è›‹ç™½è´¨ç»“æ„é¢„æµ‹
- è›‹ç™½è´¨åŠŸèƒ½æ³¨é‡Š
- è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œ
- è›‹ç™½è´¨å·¥ç¨‹ä¸è®¾è®¡
- è›‹ç™½è´¨ç»„å­¦åˆ†æ
""")

# If no documents are uploaded, display prompt
if not st.session_state.uploaded_files:
    st.info("è¯·ä¸Šä¼ è›‹ç™½è´¨ç ”ç©¶ç›¸å…³çš„PDFæ–‡çŒ®ä»¥å¼€å§‹é—®ç­”")
